{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cef31b27",
   "metadata": {},
   "source": [
    "# Text Preprocessing in Python | Set 2"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0bd5c364",
   "metadata": {},
   "source": [
    "In the previous post, we saw the basic preprocessing steps when working with textual data. In this article, we look at some more advanced text preprocessing techniques. We can use these techniques to gain more insights into the data that we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cea1c507",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/eosindo/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/eosindo/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/eosindo/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85650219",
   "metadata": {},
   "source": [
    "## Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "raw",
   "id": "48415584",
   "metadata": {},
   "source": [
    "The part of speech explains how a word is used in a sentence. In a sentence. In a sentence, a word can have different contexts and semantic meanings. The basic natural languange processing models like bag-of-words fail to identify these relations between words. Hence, we use part of speech tagging to mark a work to its part of speech tag based on its context in the data. It is also used to extract relationships between words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cfa04e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('You', 'PRP'),\n",
       " ('just', 'RB'),\n",
       " ('gave', 'VBD'),\n",
       " ('me', 'PRP'),\n",
       " ('a', 'DT'),\n",
       " ('scare', 'NN')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "# convert text into word_tokens with their tags\n",
    "def pos_tagging(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    return pos_tag(word_tokens)\n",
    "\n",
    "pos_tagging('You just gave me a scare')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fd95fab1",
   "metadata": {},
   "source": [
    "In the given example, PRP stands for personal pronoun, RB for adverg, VBD for verb past tense, DT for determiner and NN for noun. We can get the details of all the part of speech tags using the Penn Treebank tagset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f3928d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to /home/eosindo/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# download the tagset\n",
    "nltk.download('tagsets')\n",
    "\n",
    "# extract information about the tag\n",
    "nltk.help.upenn_tagset('NN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a31da0",
   "metadata": {},
   "source": [
    "### Chunking"
   ]
  },
  {
   "cell_type": "raw",
   "id": "703c698d",
   "metadata": {},
   "source": [
    "Chunking is the process of extracting phrases from unstructured text and more structure to it. It is also known as shallow parsing. It is done on top of Part of Speech tagging. It groups word into \"chunks\", mainly of noun phrases. Churking is done using regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06e14ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP the/DT little/JJ yellow/JJ bird/NN)\n",
      "  is/VBZ\n",
      "  flying/VBG\n",
      "  in/IN\n",
      "  (NP the/DT sky/NN))\n",
      "(NP the/DT little/JJ yellow/JJ bird/NN)\n",
      "(NP the/DT sky/NN)\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "# define chunking function with text and regular\n",
    "# expression representing grammar as parameter\n",
    "def chunking(text, grammar):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    \n",
    "    # label words with part of speech\n",
    "    word_pos = pos_tag(word_tokens)\n",
    "    \n",
    "    # create a chunk parser using grammar\n",
    "    chunkParser = nltk.RegexpParser(grammar)\n",
    "    \n",
    "    # test it on the list of word tokens with tagged pos\n",
    "    tree = chunkParser.parse(word_pos)\n",
    "    \n",
    "    for subtree in tree.subtrees():\n",
    "        print(subtree)\n",
    "    tree.draw()\n",
    "    \n",
    "sentence = 'the little yellow bird is flying in the sky'\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "chunking(sentence, grammar)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "942233dd",
   "metadata": {},
   "source": [
    "In the given example, grammar, which is defined using a simple regular expression rule. This rule says that an NP(Noun Phrase)chunk should be formed whenever the chunker finds an optional determiner(DT) followed by a ny number of adjectives(JJ) and then a noun(NN).\n",
    "Libraries like spaCy and Textblob are more suited for chunking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b90c8d",
   "metadata": {},
   "source": [
    "### Named Entity Recognition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1a6979",
   "metadata": {},
   "outputs": [],
   "source": [
    "Named Entity Recognition is used to extract information from unstructured text. It is used to classify entities present in a text into categories like a person, organization, event, places, etc. It gives us detailed knowledge about the text and the relationships between the different entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce11e061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Bill/NNP)\n",
      "  works/VBZ\n",
      "  for/IN\n",
      "  (ORGANIZATION GeeksforGeeks/NNP)\n",
      "  so/RB\n",
      "  he/PRP\n",
      "  went/VBD\n",
      "  to/TO\n",
      "  (GPE Delhi/NNP)\n",
      "  for/IN\n",
      "  a/DT\n",
      "  meetup/NN\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag, ne_chunk\n",
    "  \n",
    "def named_entity_recognition(text):\n",
    "    # tokenize the text\n",
    "    word_tokens = word_tokenize(text)\n",
    "  \n",
    "    # part of speech tagging of words\n",
    "    word_pos = pos_tag(word_tokens)\n",
    "  \n",
    "    # tree of word entities\n",
    "    print(ne_chunk(word_pos))\n",
    "  \n",
    "text = 'Bill works for GeeksforGeeks so he went to Delhi for a meetup.'\n",
    "named_entity_recognition(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7144abf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
